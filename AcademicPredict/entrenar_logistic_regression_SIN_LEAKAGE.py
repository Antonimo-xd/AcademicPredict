"""
===============================================================================
REGRESIÃ“N LOGÃSTICA - ENTRENAMIENTO SIN DATA LEAKAGE
===============================================================================

CONCEPTO EDUCATIVO: Â¿QuÃ© es RegresiÃ³n LogÃ­stica?
------------------------------------------------
A pesar del nombre "regresiÃ³n", es un algoritmo de CLASIFICACIÃ“N.
Es el modelo mÃ¡s SIMPLE y CLÃSICO de Machine Learning.

ANALOGÃA EDUCATIVA: La LÃ­nea Divisoria
---------------------------------------
Imagina que tienes estudiantes en un grÃ¡fico 2D:
- Eje X: Rendimiento acadÃ©mico
- Eje Y: Asistencia a clases

RegresiÃ³n LogÃ­stica busca trazar una LÃNEA que separe:
- Estudiantes que abandonan (un lado de la lÃ­nea)
- Estudiantes que continÃºan (otro lado de la lÃ­nea)

En realidad, con 43 features, Â¡es un HIPERPLANO en 43 dimensiones!

MATEMÃTICA SIMPLIFICADA:
------------------------
1. Calcula una puntuaciÃ³n lineal:
   z = w1*feature1 + w2*feature2 + ... + w43*feature43 + b
   
2. Aplica funciÃ³n sigmoide para convertir a probabilidad:
   P(abandono) = 1 / (1 + e^(-z))
   
3. Si P > 0.5 â†’ predice "abandono"
   Si P < 0.5 â†’ predice "no abandono"

VENTAJAS:
âœ… Muy rÃ¡pido de entrenar
âœ… FÃ¡cil de interpretar (coeficientes = importancia)
âœ… Poco propenso a overfitting
âœ… Funciona bien como BASELINE (referencia)

DESVENTAJAS:
âŒ Asume relaciones LINEALES (no captura patrones complejos)
âŒ Menos preciso que XGBoost o redes neuronales
âŒ No captura interacciones entre variables automÃ¡ticamente

Â¿POR QUÃ‰ LO USAMOS?
------------------
1. BASELINE: Comparar quÃ© tan mejor es XGBoost
2. INTERPRETABILIDAD: Entender relaciÃ³n directa de cada variable
3. SIMPLICIDAD: MÃ¡s fÃ¡cil de explicar a stakeholders
4. VELOCIDAD: Predicciones en milisegundos

DIFERENCIA CLAVE vs VERSIÃ“N CON LEAKAGE:
----------------------------------------
âŒ ANTES: 66 features (incluÃ­a informaciÃ³n del futuro)
âœ… AHORA: 43 features (solo informaciÃ³n disponible PRE-deserciÃ³n)

Autor: BastiÃ¡n
Fecha: Noviembre 2024
Proyecto: AcademicPredict
===============================================================================
"""

import pickle
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    classification_report, 
    confusion_matrix, 
    roc_auc_score, 
    roc_curve,
    precision_recall_curve,
    average_precision_score
)
import joblib
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# ============================================================================
# CONFIGURACIÃ“N
# ============================================================================

# Paths de archivos
TRAIN_TEST_PATH = 'modelos_ml/train_test_splits_SIN_LEAKAGE.pkl'
MODEL_OUTPUT_PATH = 'modelos_ml/logistic_regression_model_SIN_LEAKAGE.pkl'
RESULTS_DIR = 'resultados_ml/'

# ConfiguraciÃ³n del modelo
"""
EXPLICACIÃ“N DE HIPERPARÃMETROS:
-------------------------------

max_iter=1000:
  - NÃºmero mÃ¡ximo de iteraciones del optimizador
  - RegresiÃ³n LogÃ­stica usa optimizaciÃ³n iterativa
  - 1000 iteraciones es suficiente para convergencia

solver='lbfgs':
  - Algoritmo de optimizaciÃ³n
  - LBFGS = Limited-memory Broyden-Fletcher-Goldfarb-Shanno
  - Eficiente para datasets medianos/grandes
  - Alternativas: 'newton-cg', 'sag', 'saga'

class_weight='balanced':
  - Ajusta pesos automÃ¡ticamente para clases desbalanceadas
  - Clase minoritaria (abandono) recibe mÃ¡s peso
  - Equivalente a: weight = n_samples / (n_classes * n_samples_per_class)

random_state=42:
  - Semilla para reproducibilidad
  - Afecta la inicializaciÃ³n del solver

penalty='l2':
  - RegularizaciÃ³n L2 (Ridge)
  - Previene overfitting penalizando coeficientes grandes
  - L2 es la mÃ¡s comÃºn (suaviza coeficientes)

C=1.0:
  - Inverso de la fuerza de regularizaciÃ³n
  - Valores pequeÃ±os = mÃ¡s regularizaciÃ³n
  - Valores grandes = menos regularizaciÃ³n
  - 1.0 es el valor por defecto (balance)
"""

LOGREG_CONFIG = {
    'max_iter': 1000,
    'solver': 'lbfgs',
    'class_weight': 'balanced',
    'random_state': 42,
    'penalty': 'l2',
    'C': 1.0,
    'n_jobs': -1
}

print("=" * 80)
print("ğŸ“Š ENTRENAMIENTO: REGRESIÃ“N LOGÃSTICA SIN DATA LEAKAGE")
print("=" * 80)
print(f"\nğŸ“… Fecha y hora: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print(f"\nğŸ“ Cargando datos desde: {TRAIN_TEST_PATH}")

# ============================================================================
# 1. CARGAR DATOS LIMPIOS (SIN LEAKAGE)
# ============================================================================

try:
    with open(TRAIN_TEST_PATH, 'rb') as f:
        data = pickle.load(f)
    
    # Para RegresiÃ³n LogÃ­stica NO usamos datos balanceados con SMOTE
    # Usamos class_weight='balanced' en su lugar
    X_train = data['X_train']  # â† Datos originales (sin SMOTE)
    X_test = data['X_test']
    y_train = data['y_train']  # â† Labels originales (sin SMOTE)
    y_test = data['y_test']
    feature_names = data['feature_names']
    
    print(f"\nâœ… Datos cargados exitosamente")
    print(f"\nğŸ“Š ESTADÃSTICAS DEL DATASET SIN LEAKAGE:")
    print(f"   - Features totales: {len(feature_names)}")
    print(f"   - Registros train: {X_train.shape[0]:,}")
    print(f"   - Registros test: {X_test.shape[0]:,}")
    print(f"   - Balance en train: {np.sum(y_train == 1):,} abandonos / {np.sum(y_train == 0):,} no abandonos")
    print(f"   - Balance en test: {np.sum(y_test == 1):,} abandonos / {np.sum(y_test == 0):,} no abandonos")
    print(f"   - Ratio desbalance: {np.sum(y_train == 0) / np.sum(y_train == 1):.2f}:1")
    
    print(f"\nğŸ’¡ NOTA IMPORTANTE:")
    print(f"   RegresiÃ³n LogÃ­stica NO usa datos balanceados con SMOTE.")
    print(f"   En su lugar, usa class_weight='balanced' que ajusta pesos internamente.")
    print(f"   Esto evita 'inflar' artificialmente el dataset.")
    
except FileNotFoundError:
    print(f"\nâŒ ERROR: No se encontrÃ³ el archivo {TRAIN_TEST_PATH}")
    print("\nğŸ’¡ SOLUCIÃ“N: Primero debes ejecutar preparar_datos_ml_SIN_LEAKAGE.py")
    exit(1)

# ============================================================================
# 2. ENTRENAR REGRESIÃ“N LOGÃSTICA
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ¯ PASO 2: ENTRENAMIENTO DEL MODELO")
print("=" * 80)

print(f"\nğŸ”§ ConfiguraciÃ³n del modelo:")
for key, value in LOGREG_CONFIG.items():
    if key != 'n_jobs':  # Omitir n_jobs en el print
        print(f"   - {key}: {value}")

"""
CONCEPTO EDUCATIVO: Proceso de entrenamiento
--------------------------------------------
RegresiÃ³n LogÃ­stica entrena encontrando los pesos (w1, w2, ..., w43) 
que MINIMIZAN la funciÃ³n de pÃ©rdida (log loss):

1. Inicializa pesos aleatoriamente
2. FOR iteraciÃ³n = 1 to 1000:
   a. Calcula predicciones con pesos actuales
   b. Calcula log loss (error)
   c. Calcula gradiente (direcciÃ³n de mejora)
   d. Actualiza pesos en direcciÃ³n del gradiente
   e. Si converge (loss no cambia) â†’ termina
3. Devuelve pesos finales

MATEMÃTICA:
Loss = -1/N * Î£[y*log(Å·) + (1-y)*log(1-Å·)]
donde:
- y = etiqueta real (0 o 1)
- Å· = probabilidad predicha

LBFGS usa aproximaciÃ³n de segunda derivada (mÃ¡s rÃ¡pido que gradiente simple)
"""

logreg_model = LogisticRegression(**LOGREG_CONFIG)

print("\nâ³ Entrenando modelo de RegresiÃ³n LogÃ­stica...")
print("   (Esto deberÃ­a tomar menos de 1 minuto)")

logreg_model.fit(X_train, y_train)

# Verificar convergencia
if logreg_model.n_iter_[0] < LOGREG_CONFIG['max_iter']:
    print(f"âœ… Modelo convergiÃ³ en {logreg_model.n_iter_[0]} iteraciones")
else:
    print(f"âš ï¸  Modelo alcanzÃ³ mÃ¡ximo de iteraciones ({LOGREG_CONFIG['max_iter']})")
    print("   Considera aumentar max_iter si ves este mensaje")

# ============================================================================
# 3. GENERAR PREDICCIONES
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ¯ PASO 3: GENERACIÃ“N DE PREDICCIONES")
print("=" * 80)

# Predicciones en conjunto de test
y_pred = logreg_model.predict(X_test)
y_pred_proba = logreg_model.predict_proba(X_test)[:, 1]  # Probabilidad de abandono

print("\nğŸ“Š DISTRIBUCIÃ“N DE PREDICCIONES:")
print(f"   - Predichos como NO ABANDONO: {np.sum(y_pred == 0):,} ({np.sum(y_pred == 0)/len(y_pred)*100:.2f}%)")
print(f"   - Predichos como ABANDONO: {np.sum(y_pred == 1):,} ({np.sum(y_pred == 1)/len(y_pred)*100:.2f}%)")

print("\nğŸ“Š ESTADÃSTICAS DE PROBABILIDADES:")
print(f"   - Probabilidad promedio de abandono: {y_pred_proba.mean():.4f}")
print(f"   - Probabilidad mÃ­nima: {y_pred_proba.min():.4f}")
print(f"   - Probabilidad mÃ¡xima: {y_pred_proba.max():.4f}")

# ============================================================================
# 4. EVALUACIÃ“N DEL MODELO
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ“ˆ PASO 4: EVALUACIÃ“N DEL MODELO")
print("=" * 80)

# Reporte de clasificaciÃ³n
print("\nğŸ“Š REPORTE DE CLASIFICACIÃ“N:")
print("\n" + classification_report(y_test, y_pred, 
                                   target_names=['No Abandono', 'Abandono'],
                                   digits=4))

# Matriz de confusiÃ³n
cm = confusion_matrix(y_test, y_pred)

print("ğŸ“Š MATRIZ DE CONFUSIÃ“N:")
print("\n                Predicho")
print("                No Abandono  |  Abandono")
print("         " + "-" * 40)
print(f"Real No  |     {cm[0][0]:>6,}     |  {cm[0][1]:>6,}")
print(f"Real SÃ­  |     {cm[1][0]:>6,}     |  {cm[1][1]:>6,}")

# Calcular mÃ©tricas individuales
tn, fp, fn, tp = cm.ravel()
precision = tp / (tp + fp) if (tp + fp) > 0 else 0
recall = tp / (tp + fn) if (tp + fn) > 0 else 0
f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
accuracy = (tp + tn) / (tp + tn + fp + fn)

# ROC-AUC
roc_auc = roc_auc_score(y_test, y_pred_proba)
avg_precision = average_precision_score(y_test, y_pred_proba)

print(f"\nğŸ¯ MÃ‰TRICAS CLAVE:")
print(f"   - Precision: {precision:.4f} ({precision*100:.2f}%)")
print(f"   - Recall: {recall:.4f} ({recall*100:.2f}%)")
print(f"   - F1-Score: {f1:.4f}")
print(f"   - Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")
print(f"   - ROC-AUC: {roc_auc:.4f}")
print(f"   - Average Precision: {avg_precision:.4f}")

# ============================================================================
# 5. ANÃLISIS DE COEFICIENTES (FEATURE IMPORTANCE)
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ“Š PASO 5: ANÃLISIS DE COEFICIENTES")
print("=" * 80)

"""
CONCEPTO EDUCATIVO: Coeficientes en RegresiÃ³n LogÃ­stica
-------------------------------------------------------
Los coeficientes representan la IMPORTANCIA y DIRECCIÃ“N de cada feature:

MAGNITUD (valor absoluto):
- Coeficiente grande = feature muy importante
- Coeficiente pequeÃ±o = feature poco importante

SIGNO:
- Positivo (+): Aumenta probabilidad de abandono
  Ejemplo: coef_edad = +0.5 â†’ Mayor edad â†’ MÃ¡s abandono
  
- Negativo (-): Disminuye probabilidad de abandono
  Ejemplo: coef_rendimiento = -0.8 â†’ Mayor rendimiento â†’ Menos abandono

INTERPRETACIÃ“N MATEMÃTICA:
Si coef_rendimiento = -0.8:
- Por cada aumento de 1 desviaciÃ³n estÃ¡ndar en rendimiento
- Las log-odds de abandono disminuyen en 0.8
- O equivalentemente, las odds se multiplican por e^(-0.8) = 0.45

IMPORTANTE:
Los datos estÃ¡n NORMALIZADOS (StandardScaler), por eso podemos 
comparar magnitudes directamente.
"""

# Obtener coeficientes
coefficients = logreg_model.coef_[0]

# Crear DataFrame
coef_df = pd.DataFrame({
    'feature': feature_names,
    'coefficient': coefficients,
    'abs_coefficient': np.abs(coefficients)
}).sort_values('abs_coefficient', ascending=False)

print("\nğŸ” TOP 20 VARIABLES MÃS IMPORTANTES (por magnitud):\n")
print("Ranking | Variable                          | Coeficiente | InterpretaciÃ³n")
print("-" * 90)

for idx, row in coef_df.head(20).iterrows():
    coef_val = row['coefficient']
    interpretation = "â†‘ Abandono" if coef_val > 0 else "â†“ Abandono"
    print(f"{coef_df.index.get_loc(idx)+1:>3}     | {row['feature']:<32} | {coef_val:>11.4f} | {interpretation}")

# Guardar coeficientes completos
coef_path = f'{RESULTS_DIR}logistic_regression_coefficients_SIN_LEAKAGE.csv'
coef_df.to_csv(coef_path, index=False)
print(f"\nâœ… Coeficientes guardados: {coef_path}")

# Analizar signos
positive_coefs = coef_df[coef_df['coefficient'] > 0]
negative_coefs = coef_df[coef_df['coefficient'] < 0]

print(f"\nğŸ“Š DISTRIBUCIÃ“N DE COEFICIENTES:")
print(f"   - Features que AUMENTAN riesgo (+): {len(positive_coefs)}")
print(f"   - Features que DISMINUYEN riesgo (-): {len(negative_coefs)}")

print(f"\nğŸ” TOP 5 FEATURES QUE MÃS AUMENTAN RIESGO:")
for idx, row in positive_coefs.head(5).iterrows():
    print(f"   â€¢ {row['feature']}: +{row['coefficient']:.4f}")

print(f"\nğŸ” TOP 5 FEATURES QUE MÃS DISMINUYEN RIESGO:")
for idx, row in negative_coefs.head(5).iterrows():
    print(f"   â€¢ {row['feature']}: {row['coefficient']:.4f}")

# ============================================================================
# 6. VISUALIZACIONES
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ“Š PASO 6: GENERACIÃ“N DE VISUALIZACIONES")
print("=" * 80)

# Configurar estilo
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

# -------------------------
# 6.1. Matriz de ConfusiÃ³n
# -------------------------
fig, ax = plt.subplots(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['No Abandono', 'Abandono'],
            yticklabels=['No Abandono', 'Abandono'],
            cbar_kws={'label': 'NÃºmero de estudiantes'},
            ax=ax)
ax.set_xlabel('PredicciÃ³n', fontsize=12, fontweight='bold')
ax.set_ylabel('Valor Real', fontsize=12, fontweight='bold')
ax.set_title('RegresiÃ³n LogÃ­stica - Matriz de ConfusiÃ³n\n(SIN Data Leakage)', 
             fontsize=14, fontweight='bold', pad=20)

# Agregar mÃ©tricas
metrics_text = f'Precision: {precision:.2%} | Recall: {recall:.2%} | F1: {f1:.4f} | Accuracy: {accuracy:.2%} | ROC-AUC: {roc_auc:.4f}'
plt.text(0.5, -0.15, metrics_text, ha='center', transform=ax.transAxes, fontsize=9)

plt.tight_layout()
confusion_path = f'{RESULTS_DIR}logistic_regression_confusion_matrix_SIN_LEAKAGE.png'
plt.savefig(confusion_path, dpi=300, bbox_inches='tight')
print(f"âœ… Guardada: {confusion_path}")
plt.close()

# -------------------------------------
# 6.2. Coeficientes (TOP 30)
# -------------------------------------
fig, ax = plt.subplots(figsize=(12, 10))

top30 = coef_df.head(30).iloc[::-1]  # Invertir para mejor visualizaciÃ³n

# Colorear por signo
colors = ['red' if c > 0 else 'blue' for c in top30['coefficient']]

bars = ax.barh(range(len(top30)), top30['coefficient'], color=colors, 
               edgecolor='black', alpha=0.7)
ax.set_yticks(range(len(top30)))
ax.set_yticklabels(top30['feature'], fontsize=10)
ax.set_xlabel('Coeficiente', fontsize=12, fontweight='bold')
ax.set_title('RegresiÃ³n LogÃ­stica - TOP 30 Coeficientes\n(SIN Data Leakage)', 
             fontsize=14, fontweight='bold', pad=20)
ax.axvline(0, color='black', linewidth=2, linestyle='-')
ax.grid(True, axis='x', alpha=0.3)

# Leyenda
from matplotlib.patches import Patch
legend_elements = [
    Patch(facecolor='red', alpha=0.7, label='â†‘ Aumenta riesgo abandono'),
    Patch(facecolor='blue', alpha=0.7, label='â†“ Disminuye riesgo abandono')
]
ax.legend(handles=legend_elements, loc='lower right', fontsize=10)

# Agregar valores
for i, (bar, val) in enumerate(zip(bars, top30['coefficient'])):
    x_pos = val + (0.01 if val > 0 else -0.01)
    ha = 'left' if val > 0 else 'right'
    ax.text(x_pos, bar.get_y() + bar.get_height()/2, 
            f'{val:.3f}', va='center', ha=ha, fontsize=8)

plt.tight_layout()
coef_plot_path = f'{RESULTS_DIR}logistic_regression_coefficients_SIN_LEAKAGE.png'
plt.savefig(coef_plot_path, dpi=300, bbox_inches='tight')
print(f"âœ… Guardada: {coef_plot_path}")
plt.close()

# -------------------
# 6.3. Curva ROC
# -------------------
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)

fig, ax = plt.subplots(figsize=(10, 8))
ax.plot(fpr, tpr, color='darkorange', lw=2, 
        label=f'RegresiÃ³n LogÃ­stica (AUC = {roc_auc:.4f})')
ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', 
        label='Clasificador Aleatorio (AUC = 0.50)')
ax.set_xlim([0.0, 1.0])
ax.set_ylim([0.0, 1.05])
ax.set_xlabel('False Positive Rate (FPR)', fontsize=12, fontweight='bold')
ax.set_ylabel('True Positive Rate (TPR) - Recall', fontsize=12, fontweight='bold')
ax.set_title('RegresiÃ³n LogÃ­stica - Curva ROC (SIN Data Leakage)', 
             fontsize=14, fontweight='bold')
ax.legend(loc="lower right", fontsize=10)
ax.grid(True, alpha=0.3)

plt.tight_layout()
roc_path = f'{RESULTS_DIR}logistic_regression_roc_curve_SIN_LEAKAGE.png'
plt.savefig(roc_path, dpi=300, bbox_inches='tight')
print(f"âœ… Guardada: {roc_path}")
plt.close()

# -----------------------------
# 6.4. Precision-Recall Curve
# -----------------------------
precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_pred_proba)

fig, ax = plt.subplots(figsize=(10, 8))
ax.plot(recall_curve, precision_curve, color='blue', lw=2,
        label=f'RegresiÃ³n LogÃ­stica (AP = {avg_precision:.4f})')
ax.axhline(y=precision, color='red', linestyle='--', lw=2,
          label=f'Precision actual = {precision:.4f}')
ax.set_xlim([0.0, 1.0])
ax.set_ylim([0.0, 1.05])
ax.set_xlabel('Recall', fontsize=12, fontweight='bold')
ax.set_ylabel('Precision', fontsize=12, fontweight='bold')
ax.set_title('RegresiÃ³n LogÃ­stica - Curva Precision-Recall (SIN Data Leakage)', 
             fontsize=14, fontweight='bold')
ax.legend(loc="lower left", fontsize=10)
ax.grid(True, alpha=0.3)

plt.tight_layout()
pr_path = f'{RESULTS_DIR}logistic_regression_precision_recall_SIN_LEAKAGE.png'
plt.savefig(pr_path, dpi=300, bbox_inches='tight')
print(f"âœ… Guardada: {pr_path}")
plt.close()

# ----------------------------------------
# 6.5. DistribuciÃ³n de Probabilidades
# ----------------------------------------
fig, ax = plt.subplots(figsize=(12, 6))

abandono_probs = y_pred_proba[y_test == 1]
no_abandono_probs = y_pred_proba[y_test == 0]

ax.hist(no_abandono_probs, bins=50, alpha=0.6, label='No Abandono Real', 
        color='green', edgecolor='black')
ax.hist(abandono_probs, bins=50, alpha=0.6, label='Abandono Real', 
        color='red', edgecolor='black')

ax.axvline(0.5, color='black', linestyle='--', linewidth=2, 
          label='Umbral de decisiÃ³n (0.5)')
ax.set_xlabel('Probabilidad Predicha de Abandono', fontsize=12, fontweight='bold')
ax.set_ylabel('Frecuencia', fontsize=12, fontweight='bold')
ax.set_title('RegresiÃ³n LogÃ­stica - DistribuciÃ³n de Probabilidades (SIN Data Leakage)', 
             fontsize=14, fontweight='bold')
ax.legend(fontsize=10)
ax.grid(True, alpha=0.3)

plt.tight_layout()
prob_path = f'{RESULTS_DIR}logistic_regression_probability_distribution_SIN_LEAKAGE.png'
plt.savefig(prob_path, dpi=300, bbox_inches='tight')
print(f"âœ… Guardada: {prob_path}")
plt.close()

# ============================================================================
# 7. GUARDAR MODELO Y RESULTADOS
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ’¾ PASO 7: GUARDAR MODELO Y RESULTADOS")
print("=" * 80)

# Guardar modelo entrenado
joblib.dump(logreg_model, MODEL_OUTPUT_PATH)
print(f"âœ… Modelo guardado: {MODEL_OUTPUT_PATH}")

# Guardar resultados completos
results_data = {
    'y_test': y_test,
    'y_pred': y_pred,
    'y_pred_proba': y_pred_proba,
    'feature_names': feature_names,
    'coefficients': coef_df,
    'metrics': {
        'precision': precision,
        'recall': recall,
        'f1_score': f1,
        'accuracy': accuracy,
        'roc_auc': roc_auc,
        'average_precision': avg_precision,
        'confusion_matrix': cm
    },
    'config': LOGREG_CONFIG
}

results_output_path = MODEL_OUTPUT_PATH.replace('.pkl', '_results.pkl')
with open(results_output_path, 'wb') as f:
    pickle.dump(results_data, f)
print(f"âœ… Resultados guardados: {results_output_path}")

# ============================================================================
# 8. RESUMEN FINAL
# ============================================================================

print("\n" + "=" * 80)
print("âœ… ENTRENAMIENTO COMPLETADO - REGRESIÃ“N LOGÃSTICA SIN DATA LEAKAGE")
print("=" * 80)

print(f"\nğŸ“Š RESUMEN DE MÃ‰TRICAS FINALES:")
print(f"   - Precision: {precision:.4f} ({precision*100:.2f}%)")
print(f"   - Recall: {recall:.4f} ({recall*100:.2f}%)")
print(f"   - F1-Score: {f1:.4f}")
print(f"   - Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")
print(f"   - ROC-AUC: {roc_auc:.4f}")
print(f"   - Average Precision: {avg_precision:.4f}")

print(f"\nğŸ” TOP 5 COEFICIENTES MÃS IMPORTANTES:")
for idx, row in coef_df.head(5).iterrows():
    direction = "â†‘" if row['coefficient'] > 0 else "â†“"
    print(f"   {coef_df.index.get_loc(idx)+1}. {row['feature']}: {row['coefficient']:.4f} {direction}")

print(f"\nğŸ’¾ ARCHIVOS GENERADOS:")
print(f"   âœ… {MODEL_OUTPUT_PATH}")
print(f"   âœ… {results_output_path}")
print(f"   âœ… {confusion_path}")
print(f"   âœ… {coef_plot_path}")
print(f"   âœ… {coef_path}")
print(f"   âœ… {roc_path}")
print(f"   âœ… {pr_path}")
print(f"   âœ… {prob_path}")

print(f"\nğŸ’¡ ROL DE REGRESIÃ“N LOGÃSTICA:")
print(f"   âœ… BASELINE: Modelo simple para comparaciÃ³n")
print(f"   âœ… INTERPRETABILIDAD: FÃ¡cil de explicar a stakeholders")
print(f"   âœ… VELOCIDAD: Predicciones instantÃ¡neas")
print(f"   âœ… ROBUSTEZ: Menos propenso a overfitting")
print(f"   ")
print(f"   Aunque probablemente XGBoost tenga mejores mÃ©tricas,")
print(f"   RegresiÃ³n LogÃ­stica es valioso para:")
print(f"   - Entender relaciÃ³n directa de cada variable")
print(f"   - Validar que el problema es predecible")
print(f"   - Deployment en sistemas con recursos limitados")

print("\n" + "=" * 80)
print("ğŸ‰ Â¡PROCESO FINALIZADO!")
print("=" * 80)
print("\nâœ… LOS 3 MODELOS HAN SIDO ENTRENADOS SIN DATA LEAKAGE")
print("\nPrÃ³ximo paso: Comparar los 3 modelos y elegir el mejor para producciÃ³n")